# Dot-Product Attention

[A2-Nets: Double Attention Networks](https://arxiv.org/abs/1810.11579) (with **linear** computational complexity and memory consumption of [Non-local Neural Networks](https://arxiv.org/abs/1711.07971))

[Local Relation Networks for Image Recognition](https://arxiv.org/abs/1904.11491) (shows that scalar attention performs best)